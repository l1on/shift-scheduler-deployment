groups:
  - name: shifts-scheduler
    rules:
    - record: shifts_scheduler:http_request_duration_seconds:avg
      expr: shifts_scheduler_http_request_duration_seconds_sum / shifts_scheduler_http_request_duration_seconds_count
    - alert: LatencyHigh
      expr: shifts_scheduler:http_request_duration_seconds:avg > 1
      for: 5m
      labels: 
        severity: warning
      annotations:
        description: the shifts-scheduler has a latency of {{$value}} seconds
          for {{$labels.method}} {{$labels.handler}}
        summary: shifts-scheduler high latency
  # the node.rules are mostly copied from coreos/kube-prometheus/charts/exporter-node/templates/node.rules.yaml
  # so that we can customize it here to reduce the false alarming rates of NodeDiskRunningFull. This hacky stuff
  # should be removed once coreOS promotes its kube-prometheus chart into the offical Helm chart repo. For details,
  # see: https://github.com/coreos/prometheus-operator/issues/1180
  - name: node.rules
    rules:
    - record: instance:node_cpu:rate:sum
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[3m]))
        BY (instance)
    - record: instance:node_filesystem_usage:sum
      expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
        BY (instance)
    - record: instance:node_network_receive_bytes:rate:sum
      expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
    - record: instance:node_network_transmit_bytes:rate:sum
      expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
    - record: instance:node_cpu:ratio
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode) / ON(instance)
        GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
    - record: cluster:node_cpu:sum_rate5m
      expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m]))
    - record: cluster:node_cpu:ratio
      expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
    - alert: NodeExporterDown
      expr: absent(up{job="node-exporter"} == 1)
      for: 10m
      labels:
        severity: warning
      annotations:
        description: Prometheus could not scrape a node-exporter for more than 10m,
          or node-exporters have disappeared from discovery
        summary: Prometheus could not scrape a node-exporter
    # customized NodeDiskRunningFull alert rules
    - record: 'node:node_filesystem_usage:'
      expr: max by (namespace, pod, device, instance) ((node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"} - node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs"}) / node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"})
    - record: 'node:node_filesystem_avail:'
      expr:  max by (namespace, pod, device, instance) (node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs"} / node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"})
    - alert: NodeDiskRunningFull
      expr: '(node:node_filesystem_usage: > 0.90) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)'
      for: 30m
      labels:
        severity: warning
      annotations:
        description: Device {{`{{$labels.device}}`}} on node {{`{{$labels.instance}}`}} may be running
          full within the next 24 hours
        summary: Node disk is running full within 24 hours
    - alert: NodeDiskRunningFull
      expr: '(node:node_filesystem_usage: > 0.95) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)'
      for: 10m
      labels:
        severity: critical
      annotations:
        description: Device {{`{{$labels.device}}`}} on node {{`{{$labels.instance}}`}} is likely to be
          filled up within the next 2 hours
        summary: Node disk is running full within 2 hours
